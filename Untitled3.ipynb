{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "\n",
    "import os\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ENV_NAME = 'Breakout-v0'  # Environment name\n",
    "FRAME_WIDTH = 84  # Resized frame width\n",
    "FRAME_HEIGHT = 84  # Resized frame height\n",
    "NUM_EPISODES = 12000  # Number of episodes the agent plays\n",
    "STATE_LENGTH = 4  # Number of most recent frames to produce the input to the network\n",
    "GAMMA = 0.99  # Discount factor\n",
    "EXPLORATION_STEPS = 1000000  # Number of steps over which the initial value of epsilon is linearly annealed to its final value\n",
    "INITIAL_EPSILON = 1.0  # Initial value of epsilon in epsilon-greedy\n",
    "FINAL_EPSILON = 0.1  # Final value of epsilon in epsilon-greedy\n",
    "INITIAL_REPLAY_SIZE = 20000  # Number of steps to populate the replay memory before training starts\n",
    "NUM_REPLAY_MEMORY = 400000  # Number of replay memory the agent uses for training\n",
    "BATCH_SIZE = 32  # Mini batch size\n",
    "TARGET_UPDATE_INTERVAL = 10000  # The frequency with which the target network is updated\n",
    "TRAIN_INTERVAL = 4  # The agent selects 4 actions between successive updates\n",
    "LEARNING_RATE = 0.00025  # Learning rate used by RMSProp\n",
    "MOMENTUM = 0.95  # Momentum used by RMSProp\n",
    "MIN_GRAD = 0.01  # Constant added to the squared gradient in the denominator of the RMSProp update\n",
    "SAVE_INTERVAL = 300000  # The frequency with which the network is saved\n",
    "NO_OP_STEPS = 30  # Maximum number of \"do nothing\" actions to be performed by the agent at the start of an episode\n",
    "LOAD_NETWORK = False\n",
    "TRAIN = True\n",
    "SAVE_NETWORK_PATH = 'saved_networks/' + ENV_NAME\n",
    "SAVE_SUMMARY_PATH = 'graphs/dqn/kerastestnew/2'\n",
    "NUM_EPISODES_AT_TEST = 30  # Number of episodes the agent plays at test time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(observation, last_observation):\n",
    "    processed_observation = np.maximum(observation, last_observation)\n",
    "    processed_observation = np.uint8(resize(rgb2gray(processed_observation), (FRAME_WIDTH, FRAME_HEIGHT)) * 255)\n",
    "    return processed_observation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "env = gym.make(ENV_NAME)\n",
    "num_actions = env.action_space.n\n",
    "epsilon = INITIAL_EPSILON\n",
    "epsilon_step = (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORATION_STEPS\n",
    "t = 0\n",
    "\n",
    "# Parameters used for summary\n",
    "total_reward = 0\n",
    "total_q_max = 0\n",
    "total_loss = 0\n",
    "duration = 0\n",
    "episode = 0\n",
    "\n",
    "# Create replay memory\n",
    "replay_memory = deque()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_network():\n",
    "#     model = tf.contrib.keras.models.Sequential()\n",
    "#     #Conv2D(32, (8, 8), input_shape=(4, 84, 84..., strides=(4, 4), activation=\"relu\")`\n",
    "#     model.add(tf.contrib.keras.layers.Convolution2D(32, (8, 8), strides=(4, 4), activation='relu', input_shape=(FRAME_WIDTH, FRAME_HEIGHT, STATE_LENGTH)))\n",
    "#     model.add(tf.contrib.keras.layers.Convolution2D(64, (4, 4), strides=(2, 2), activation='relu'))\n",
    "#     model.add(tf.contrib.keras.layers.Convolution2D(64, (3, 3), strides=(1, 1), activation='relu'))\n",
    "#     model.add(tf.contrib.keras.layers.Flatten())\n",
    "#     model.add(tf.contrib.keras.layers.Dense(512, activation='relu'))\n",
    "#     model.add(tf.contrib.keras.layers.Dense(num_actions))\n",
    "\n",
    "#     s = tf.placeholder(tf.float32, [None, FRAME_WIDTH, FRAME_HEIGHT, STATE_LENGTH])\n",
    "#     q_values = model(s)\n",
    "#     return s, q_values, model\n",
    "def build_network(name):\n",
    "    with tf.variable_scope(name):\n",
    "        s = tf.placeholder(tf.float32, [None, FRAME_WIDTH, FRAME_HEIGHT, STATE_LENGTH])\n",
    "        conv1 = tf.layers.conv2d(s, 32, (8,8), strides=(4,4), activation=tf.nn.relu)\n",
    "        conv2 = tf.layers.conv2d(conv1, 64, (4,4), strides=(2,2), activation=tf.nn.relu)\n",
    "        conv3 = tf.layers.conv2d(conv2, 64, (3,3), strides=(1,1), activation=tf.nn.relu)\n",
    "        flattened = tf.contrib.layers.flatten(conv3)\n",
    "        print(flattened)\n",
    "        dense1 = tf.layers.dense(flattened, 512, activation=tf.nn.relu)\n",
    "        q_values = tf.layers.dense(dense1, num_actions)\n",
    "    trainable_parameters = [v for v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES) if v.name.startswith(name)]\n",
    "    print([v.name for v in trainable_parameters])\n",
    "    return s, q_values, trainable_parameters\n",
    "\n",
    "# Create q network\n",
    "s, q_values, q_network_weights = build_network(\"q_network\")\n",
    "#s, q_values, q_network = build_network()\n",
    "#q_network_weights = q_network.trainable_weights\n",
    "\n",
    "# Create target network\n",
    "st, target_q_values, target_network_weights = build_network(\"target_network\")\n",
    "# st, target_q_values, target_network = build_network()\n",
    "# target_network_weights = target_network.trainable_weights\n",
    "\n",
    "# Define target network update operation\n",
    "update_target_network = [target_network_weights[i].assign(q_network_weights[i]) for i in range(len(target_network_weights))]\n",
    "\n",
    "\n",
    "a = tf.placeholder(tf.int64, [None])\n",
    "y = tf.placeholder(tf.float32, [None])\n",
    "\n",
    "# Convert action to one hot vector\n",
    "a_one_hot = tf.one_hot(a, num_actions, 1.0, 0.0)\n",
    "q_value = tf.reduce_sum(tf.multiply(q_values, a_one_hot), reduction_indices=1)\n",
    "\n",
    "# Clip the error, the loss is quadratic when the error is in (-1, 1), and linear outside of that region\n",
    "error = tf.abs(y - q_value)\n",
    "quadratic_part = tf.clip_by_value(error, 0.0, 1.0)\n",
    "linear_part = error - quadratic_part\n",
    "loss = tf.reduce_mean(0.5 * tf.square(quadratic_part) + linear_part)\n",
    "\n",
    "optimizer = tf.train.RMSPropOptimizer(LEARNING_RATE, momentum=MOMENTUM, epsilon=MIN_GRAD)\n",
    "grads_update = optimizer.minimize(loss, var_list=q_network_weights)\n",
    "\n",
    "def get_initial_state(observation, last_observation):\n",
    "    processed_observation = np.maximum(observation, last_observation)\n",
    "    processed_observation = np.uint8(resize(rgb2gray(processed_observation), (FRAME_WIDTH, FRAME_HEIGHT)) * 255)\n",
    "    state = [processed_observation for _ in range(STATE_LENGTH)]\n",
    "    return np.dstack(state)\n",
    "\n",
    "def get_action(state, epsilon):\n",
    "    if epsilon >= random.random() or t < INITIAL_REPLAY_SIZE:\n",
    "        action = random.randrange(num_actions)\n",
    "    else:\n",
    "        action = np.argmax(q_values.eval(feed_dict={s: [np.float32(state / 255.0)]}))\n",
    "\n",
    "    # Anneal epsilon linearly over time\n",
    "    if epsilon > FINAL_EPSILON and t >= INITIAL_REPLAY_SIZE:\n",
    "        epsilon -= epsilon_step\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver(q_network_weights)\n",
    "\n",
    "### SUMMARY STUFF\n",
    "episode_total_reward = tf.Variable(0.)\n",
    "tf.summary.scalar('atari/total_reward', episode_total_reward)\n",
    "episode_avg_max_q = tf.Variable(0.)\n",
    "tf.summary.scalar('atari/average_maxq', episode_avg_max_q)\n",
    "episode_duration = tf.Variable(0.)\n",
    "tf.summary.scalar('atari/duration', episode_duration)\n",
    "episode_avg_loss = tf.Variable(0.)\n",
    "tf.summary.scalar('atari/loss', episode_avg_loss)\n",
    "\n",
    "summary_vars = [episode_total_reward, episode_avg_max_q, episode_duration, episode_avg_loss]\n",
    "summary_placeholders = [tf.placeholder(tf.float32) for _ in range(len(summary_vars))]\n",
    "update_ops = [summary_vars[i].assign(summary_placeholders[i]) for i in range(len(summary_vars))]\n",
    "summary_op = tf.summary.merge_all()\n",
    "\n",
    "summary_writer = tf.summary.FileWriter(SAVE_SUMMARY_PATH, sess.graph)\n",
    "\n",
    "if not os.path.exists(SAVE_NETWORK_PATH):\n",
    "    os.makedirs(SAVE_NETWORK_PATH)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Initialize target network\n",
    "sess.run(update_target_network)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "\n",
    "\n",
    "def run(self, state, action, reward, terminal, observation):\n",
    "    next_state = np.dstack((state[:, :, 1:], observation))\n",
    "\n",
    "    # Clip all positive rewards at 1 and all negative rewards at -1, leaving 0 rewards unchanged\n",
    "    reward = np.clip(reward, -1, 1)\n",
    "\n",
    "    # Store transition in replay memory\n",
    "    self.replay_memory.append((state, action, reward, next_state, terminal))\n",
    "    if len(self.replay_memory) > NUM_REPLAY_MEMORY:\n",
    "        self.replay_memory.popleft()\n",
    "\n",
    "    if self.t >= INITIAL_REPLAY_SIZE:\n",
    "        # Train network\n",
    "        if self.t % TRAIN_INTERVAL == 0:\n",
    "            self.train_network()\n",
    "\n",
    "        # Update target network\n",
    "        if self.t % TARGET_UPDATE_INTERVAL == 0:\n",
    "            self.sess.run(self.update_target_network)\n",
    "\n",
    "        # Save network\n",
    "        if self.t % SAVE_INTERVAL == 0:\n",
    "            save_path = self.saver.save(self.sess, SAVE_NETWORK_PATH + '/' + ENV_NAME, global_step=self.t)\n",
    "            print('Successfully saved: ' + save_path)\n",
    "\n",
    "    self.total_reward += reward\n",
    "    self.total_q_max += np.max(self.q_values.eval(feed_dict={self.s: [np.float32(state / 255.0)]}))\n",
    "    self.duration += 1\n",
    "\n",
    "    if terminal:\n",
    "        # Write summary\n",
    "        if self.t >= INITIAL_REPLAY_SIZE:\n",
    "            stats = [self.total_reward, self.total_q_max / float(self.duration),\n",
    "                    self.duration, self.total_loss / (float(self.duration) / float(TRAIN_INTERVAL))]\n",
    "            for i in range(len(stats)):\n",
    "                self.sess.run(self.update_ops[i], feed_dict={\n",
    "                    self.summary_placeholders[i]: float(stats[i])\n",
    "                })\n",
    "            summary_str = self.sess.run(self.summary_op)\n",
    "            self.summary_writer.add_summary(summary_str, self.episode + 1)\n",
    "\n",
    "        # Debug\n",
    "        if self.t < INITIAL_REPLAY_SIZE:\n",
    "            mode = 'random'\n",
    "        elif INITIAL_REPLAY_SIZE <= self.t < INITIAL_REPLAY_SIZE + EXPLORATION_STEPS:\n",
    "            mode = 'explore'\n",
    "        else:\n",
    "            mode = 'exploit'\n",
    "        print('EPISODE: {0:6d} / TIMESTEP: {1:8d} / DURATION: {2:5d} / EPSILON: {3:.5f} / TOTAL_REWARD: {4:3.0f} / AVG_MAX_Q: {5:2.4f} / AVG_LOSS: {6:.5f} / MODE: {7}'.format(\n",
    "            self.episode + 1, self.t, self.duration, self.epsilon,\n",
    "            self.total_reward, self.total_q_max / float(self.duration),\n",
    "            self.total_loss / (float(self.duration) / float(TRAIN_INTERVAL)), mode))\n",
    "\n",
    "        self.total_reward = 0\n",
    "        self.total_q_max = 0\n",
    "        self.total_loss = 0\n",
    "        self.duration = 0\n",
    "        self.episode += 1\n",
    "\n",
    "    self.t += 1\n",
    "\n",
    "    return next_state\n",
    "\n",
    "def train_network(replay_memory):\n",
    "    state_batch = []\n",
    "    action_batch = []\n",
    "    reward_batch = []\n",
    "    next_state_batch = []\n",
    "    terminal_batch = []\n",
    "    y_batch = []\n",
    "\n",
    "    # Sample random minibatch of transition from replay memory\n",
    "    minibatch = random.sample(replay_memory, BATCH_SIZE)\n",
    "    for data in minibatch:\n",
    "        state_batch.append(data[0])\n",
    "        action_batch.append(data[1])\n",
    "        reward_batch.append(data[2])\n",
    "        next_state_batch.append(data[3])\n",
    "        terminal_batch.append(data[4])\n",
    "\n",
    "    # Convert True to 1, False to 0\n",
    "    terminal_batch = np.array(terminal_batch) + 0\n",
    "\n",
    "    target_q_values_batch = target_q_values.eval(feed_dict={st: np.float32(np.array(next_state_batch) / 255.0)})\n",
    "    y_batch = reward_batch + (1 - terminal_batch) * GAMMA * np.max(target_q_values_batch, axis=1)\n",
    "\n",
    "    local_loss, _ = sess.run([loss, grads_update], feed_dict={\n",
    "        s: np.float32(np.array(state_batch) / 255.0),\n",
    "        a: action_batch,\n",
    "        y: y_batch\n",
    "    })\n",
    "    global total_loss\n",
    "    \n",
    "    total_loss += local_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "for _ in range(NUM_EPISODES):\n",
    "    terminal = False\n",
    "    observation = env.reset()\n",
    "    for _ in range(random.randint(1, NO_OP_STEPS)):\n",
    "        last_observation = observation\n",
    "        observation, _, _, _ = env.step(0)  # Do nothing\n",
    "    state = get_initial_state(observation, last_observation)\n",
    "    while not terminal:\n",
    "        last_observation = observation\n",
    "        action = get_action(state, epsilon)\n",
    "        observation, reward, terminal, _ = env.step(action)\n",
    "        # env.render()\n",
    "        processed_observation = preprocess(observation, last_observation)\n",
    "\n",
    "#         state = agent.run(state, action, reward, terminal, processed_observation)\n",
    "        next_state = np.dstack((state[:, :, 1:], processed_observation))\n",
    "\n",
    "        # Clip all positive rewards at 1 and all negative rewards at -1, leaving 0 rewards unchanged\n",
    "        reward = np.clip(reward, -1, 1)\n",
    "\n",
    "        # Store transition in replay memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "        if len(replay_memory) > NUM_REPLAY_MEMORY:\n",
    "            replay_memory.popleft()\n",
    "\n",
    "        if t >= INITIAL_REPLAY_SIZE:\n",
    "            # Train network\n",
    "            if t % TRAIN_INTERVAL == 0:\n",
    "                train_network(replay_memory)\n",
    "\n",
    "            # Update target network\n",
    "            if t % TARGET_UPDATE_INTERVAL == 0:\n",
    "                sess.run(update_target_network)\n",
    "\n",
    "        total_reward += reward\n",
    "        total_q_max += np.max(q_values.eval(feed_dict={s: [np.float32(state / 255.0)]}))\n",
    "        duration += 1\n",
    "\n",
    "        if terminal:\n",
    "            # Write summary\n",
    "            if t >= INITIAL_REPLAY_SIZE:\n",
    "                stats = [total_reward, total_q_max / float(duration),\n",
    "                        duration, total_loss / (float(duration) / float(TRAIN_INTERVAL))]\n",
    "                for i in range(len(stats)):\n",
    "                    sess.run(update_ops[i], feed_dict={\n",
    "                        summary_placeholders[i]: float(stats[i])\n",
    "                    })\n",
    "                summary_str = sess.run(summary_op)\n",
    "                summary_writer.add_summary(summary_str, episode + 1)\n",
    "\n",
    "            # Debug\n",
    "            if t < INITIAL_REPLAY_SIZE:\n",
    "                mode = 'random'\n",
    "            elif INITIAL_REPLAY_SIZE <= t < INITIAL_REPLAY_SIZE + EXPLORATION_STEPS:\n",
    "                mode = 'explore'\n",
    "            else:\n",
    "                mode = 'exploit'\n",
    "            print('EPISODE: {0:6d} / TIMESTEP: {1:8d} / DURATION: {2:5d} / EPSILON: {3:.5f} / TOTAL_REWARD: {4:3.0f} / AVG_MAX_Q: {5:2.4f} / AVG_LOSS: {6:.5f} / MODE: {7}'.format(\n",
    "                episode + 1, t, duration, epsilon,\n",
    "                total_reward, total_q_max / float(duration),\n",
    "                total_loss / (float(duration) / float(TRAIN_INTERVAL)), mode))\n",
    "\n",
    "            total_reward = 0\n",
    "            total_q_max = 0\n",
    "            total_loss = 0\n",
    "            duration = 0\n",
    "            episode += 1\n",
    "\n",
    "        t += 1\n",
    "\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FRAMES_TEST = 300\n",
    "frames = list()\n",
    "\n",
    "observation = env.reset()\n",
    "\n",
    "last_observation = observation\n",
    "observation, _, _, _ = env.step(1) \n",
    "state = get_initial_state(observation, last_observation)\n",
    "for _ in range(MAX_FRAMES_TEST):\n",
    "    last_observation = observation\n",
    "\n",
    "    action = np.argmax(q_values.eval(feed_dict={s: [np.float32(state / 255.0)]}))\n",
    "    observation, _, terminal, _ = env.step(action)\n",
    "    \n",
    "    frames.append(observation)\n",
    "\n",
    "   # env.render()\n",
    "    processed_observation = preprocess(observation, last_observation)\n",
    "    state = np.dstack((state[:, :, 1:], processed_observation))\n",
    "    #state = np.append(state[1:, :, :], processed_observation, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "import matplotlib.pyplot as plt\n",
    "def display_frames_as_gif(frames, filename_gif = None):\n",
    "    \"\"\"\n",
    "    Displays a list of frames as a gif, with controls\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "    if filename_gif: \n",
    "        anim.save(filename_gif, writer = 'imagemagick', fps=20)\n",
    "    display(display_animation(anim, default_mode='loop'))\n",
    "display_frames_as_gif(frames[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "obs, _, _, _ = env.step(1)\n",
    "%matplotlib inline\n",
    "plt.imshow(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
