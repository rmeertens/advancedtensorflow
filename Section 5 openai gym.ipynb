{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5: deep reinforcement learning with the openai gym\n",
    "\n",
    "Reinforcement learning is the process of teaching an autonomous agent how to behave in an environment. Rather than telling the agent what action to execute at each moment you let the agent decide for himself what to do, and you give it rewards based on how it performs. \n",
    "\n",
    "Recently there is a lot of attention on reinforcement learning. One reason is that the company [OpenAI](https://en.wikipedia.org/wiki/OpenAI) released a set of environments you can use to train your agent. The environment we will be using is the OpenAI gym. There are many tasks in this package which have defined how much reward you get for each action. Take a look at the [environments that are included with the OpenAI gym](https://gym.openai.com/envs/). As you can see there are classic control problems, but OpenAI also has a lot of Atari games you can try to master!\n",
    "\n",
    "# Video 1: getting started with the OpenAI gym\n",
    "### Installing the OpenAI gym\n",
    "There are multiple ways you can install the OpenAI gym environment: \n",
    "\n",
    "##### In our Docker image\n",
    "It is already installed in our Docker image. This means you don't have to do anything to include it, and you can start playing immediately. The downside of this is that there is no screen attached to our Docker image, which gives a bit of hassle inline. \n",
    "\n",
    "##### On your own pc. \n",
    "If you do want to see a screen with your game you would have to install everything locally. This means you have your own version of Python, TensorFlow, Matplotlib, etc. installed. If this is all fine you can install the OpenAI gym with these commands: \n",
    "> `git clone https://github.com/openai/gym.git`\n",
    "\n",
    "> `cd gym`\n",
    "\n",
    "> `pip install -e .`\n",
    "\n",
    "For more information: see [this site](https://github.com/openai/gym#installing-everything). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Starting an environment\n",
    "Let's start by loading some libraries we want to use during this section. They are the normal ones you expect by now (TensorFlow, NumPy, etc...). We also load gym!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import display\n",
    "\n",
    "import gym\n",
    "\n",
    "## Set logging dir for Tensorboard\n",
    "logging_dir_n = 141\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load an environment you call the `gym.make` function and pass the name of the environment you want to load. After you reset you get the observation. If you installed gym on your local pc you can get a screen with the state of your environment using the command `env.render()`. As I will be doing everything from this Docker image we render the frame as RGB array and display it with Matplotlib..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'base' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-f0c9d51b5988>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CartPole-v0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfirstframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirstframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/gym/core.py\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassic_control\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartwidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartwidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartheight\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartheight\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Error occured while running `from pyglet.gl import *`\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HINT: make sure you have OpenGL install. On Ubuntu, you can run 'apt-get install python-opengl'. If you're running on a server, you may need a virtual frame buffer; something like this should work: 'xvfb-run -s \\\"-screen 0 1400x900x24\\\" python <your_script.py>'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcarbon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCarbonConfig\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;31m# XXX remove\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'base' is not defined"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "observation = env.reset()\n",
    "firstframe = env.render(mode = 'rgb_array')\n",
    "plt.imshow(firstframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing (random) actions\n",
    "With the function `env.action_space.sample()` you get a random action that's possible for this action space. Please take some time to figure out what kind of commands are possible, and what env.step does. \n",
    "\n",
    "In the following code we execute a few actions, plot the frames, and print what the environment tells us about the current state of the environment..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation: [-0.0263599   0.23144936  0.01105679 -0.28842569] reward: 1 done: 0 info: {}\n",
      "Observation: [-0.02173092  0.03617149  0.00528828  0.00772384] reward: 1 done: 0 info: {}\n",
      "Observation: [-0.02100749 -0.1590259   0.00544275  0.30207058] reward: 1 done: 0 info: {}\n",
      "Observation: [-0.024188    0.03601806  0.01148416  0.01110913] reward: 1 done: 0 info: {}\n",
      "Observation: [-0.02346764 -0.15926669  0.01170635  0.30739321] reward: 1 done: 0 info: {}\n",
      "Observation: [-0.02665298 -0.35455348  0.01785421  0.60374491] reward: 1 done: 0 info: {}\n",
      "Observation: [-0.03374405 -0.54992052  0.02992911  0.90199763] reward: 1 done: 0 info: {}\n",
      "Observation: [-0.04474246 -0.74543488  0.04796906  1.20393567] reward: 1 done: 0 info: {}\n",
      "Observation: [-0.05965115 -0.94114299  0.07204778  1.51125765] reward: 1 done: 0 info: {}\n",
      "Observation: [-0.07847401 -0.74696397  0.10227293  1.24190848] reward: 1 done: 0 info: {}\n",
      "Observation: [-0.09341329 -0.94323914  0.1271111   1.56479869] reward: 1 done: 0 info: {}\n",
      "Observation: [-0.11227808 -0.74984515  0.15840707  1.31431819] reward: 1 done: 0 info: {}\n",
      "Observation: [-0.12727498 -0.55704237  0.18469344  1.07511207] reward: 1 done: 0 info: {}\n",
      "Observation: [-0.13841583 -0.75405999  0.20619568  1.41960405] reward: 1 done: 0 info: {}\n",
      "Observation: [-0.15349703 -0.95104925  0.23458776  1.76902154] reward: 1 done: 1 info: {}\n",
      "Observation: [-0.17251801 -0.75921826  0.26996819  1.55749432] reward: 0 done: 1 info: {}\n",
      "Observation: [-0.18770238 -0.56814592  0.30111808  1.35967698] reward: 0 done: 1 info: {}\n",
      "Observation: [-0.19906529 -0.37777645  0.32831161  1.17416808] reward: 0 done: 1 info: {}\n",
      "Observation: [-0.20662082 -0.18804158  0.35179498  0.99956582] reward: 0 done: 1 info: {}\n",
      "Observation: [-0.21038165 -0.3857491   0.37178629  1.37927195] reward: 0 done: 1 info: {}\n",
      "Observation: [-0.21809664 -0.19663314  0.39937173  1.22178302] reward: 0 done: 1 info: {}\n",
      "Observation: [-0.2220293  -0.39414143  0.42380739  1.60905007] reward: 0 done: 1 info: {}\n",
      "Observation: [-0.22991213 -0.20568861  0.45598839  1.47228232] reward: 0 done: 1 info: {}\n",
      "Observation: [-0.2340259  -0.40275331  0.48543404  1.86713998] reward: 0 done: 1 info: {}\n",
      "Observation: [-0.24208097 -0.21495456  0.52277684  1.75516386] reward: 0 done: 1 info: {}\n",
      "Observation: [-0.24638006 -0.02794728  0.55788012  1.65890976] reward: 0 done: 1 info: {}\n",
      "Observation: [-0.246939   -0.22406727  0.59105831  2.06412631] reward: 0 done: 1 info: {}\n",
      "Observation: [-0.25142035 -0.03750377  0.63234084  1.99558499] reward: 0 done: 1 info: {}\n",
      "Observation: [-0.25217042  0.14832724  0.67225254  1.94449935] reward: 0 done: 1 info: {}\n",
      "Observation: [-0.24920388 -0.04597178  0.71114252  2.35562327] reward: 0 done: 1 info: {}\n"
     ]
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "for _ in range(30):\n",
    "    frame = env.render(mode = 'rgb_array')\n",
    "    plt.imshow(frame)\n",
    "    plt.show()\n",
    "    observation, reward, done, info = env.step(env.action_space.sample()) \n",
    "    print(\"Observation: %s reward: %d done: %d info: %s\" % (str(observation), reward, done, info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input and output \n",
    "\n",
    "Let's take a look at what we can use to train an autonomous agent: \n",
    "\n",
    "##### Observation\n",
    "The observation consists of the angle of the pole, turn velocity of the pole, the location of the base, and the movespeed of the base. Other environments have different inputs. Some environments (the Atari games) simply give you an image, other environments give a representation of the state. \n",
    "\n",
    "##### Reward\n",
    "Reward is the reward our environment gives us each frame. For cartpole the reward is 1.0 while we managed to keep the stick in less than a small angle. After we reached this angle the reward will change to 0.0 while we did not reset the environment. \n",
    "\n",
    "Other environments give different rewards. Mountaincar for example gives a reward of 0.0 while we did not reach the top, and 1.0 once we reached it. Whenever you try a new environment make sure you know when your algorithm receives a reward as this is vital for learning something. \n",
    "\n",
    "##### Done\n",
    "If this boolean is True it means you probably want to reset your network. In the case of cartpole it means the cart can't get anymore points: either by:\n",
    "- failing to keep the pole at less than a certain angle\n",
    "- failing to keep the base at less than a certain distance\n",
    "- succeeding at this for more than 200 frames!\n",
    "\n",
    "Other environments end when your lives are gone, others end after a certain amount of iterations.\n",
    "\n",
    "#### Actions\n",
    "What actions you can perform depend on the environment of you agent. In this environment you can only perform two actions: apply a force to the left of the base, or to the right of the base. In other environments, like the Atari environment, you can press several buttons!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing yourself\n",
    "Now you are able to play a pretty boring game with a text-interface..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# observation = env.reset()\n",
    "# done = False\n",
    "# while not done:\n",
    "#     frame = env.render(mode = 'rgb_array')\n",
    "#     plt.imshow(frame)\n",
    "#     plt.show()\n",
    "    \n",
    "#     useraction = -1\n",
    "#     while useraction < 0:\n",
    "#         try:\n",
    "#             useraction = int(input(\"Left (0) or right (1)?\"))\n",
    "#         except:\n",
    "#             pass\n",
    "        \n",
    "        \n",
    "#     observation, reward, done, info = env.step(useraction) \n",
    "# print(\"You are dead..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video 2: Random search\n",
    "One way you could approach this problem is with a very simple single-layer neural network that takes the input and has weights to two actions. You can imagine that this neural network makes sure the left action is chosen if the stick is too far to the left, and the right action is chosen if the stick is too far to the right. \n",
    "\n",
    "Let's start with this simple single-layer neural network. As code I took the example from [this website](http://kvfrans.com/simple-algoritms-for-solving-cartpole/). If you like and have time I would definitely explore all samples of that blogpost as it gives a lot of insight in reinforcement learning and the cartpole problem!\n",
    "\n",
    "For now it is enough to say that we can construct a single-layer neural network by having a matrix multiplication of the state with a 4x1 matrix (if the result is smaller than 0 we push left!). To get this matrix we are going to do generate A LOT of random matrices and see which ones perform best. This technique, random search, is described on the blog I linked to earlier. You can copy-paste the code from the blog, and change it however you like it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution not yet found\n",
      "Solution not yet found\n",
      "WOW! FOUND A SOLUTION TO THIS PROBLEM USING RANDOM SEARCH AT ITERATION 2!\n"
     ]
    }
   ],
   "source": [
    "def run_episode(env, parameters):  \n",
    "    observation = env.reset()\n",
    "    totalreward = 0\n",
    "    for _ in range(200):\n",
    "        action = 0 if np.matmul(parameters,observation) < 0 else 1\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        totalreward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return totalreward\n",
    "\n",
    "\n",
    "bestparams = None  \n",
    "bestreward = 0  \n",
    "for iteration in range(10000):  \n",
    "    parameters = np.random.rand(4) * 2 - 1\n",
    "    reward = run_episode(env,parameters)\n",
    "    if reward > bestreward:\n",
    "        bestreward = reward\n",
    "        bestparams = parameters\n",
    "        # considered solved if the agent lasts 200 timesteps\n",
    "        if reward == 200:\n",
    "            print(\"WOW! FOUND A SOLUTION TO THIS PROBLEM USING RANDOM SEARCH AT ITERATION %d!\" % (iteration))\n",
    "            break\n",
    "    print(\"Solution not yet found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to understand what is happening in the run_episode function, and what is happening in the search function above. After we did this it's time to visualise the best, winning, parameters: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'base' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-3805543c89b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/gym/core.py\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassic_control\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartwidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartwidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartheight\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartheight\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Error occured while running `from pyglet.gl import *`\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HINT: make sure you have OpenGL install. On Ubuntu, you can run 'apt-get install python-opengl'. If you're running on a server, you may need a virtual frame buffer; something like this should work: 'xvfb-run -s \\\"-screen 0 1400x900x24\\\" python <your_script.py>'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcarbon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCarbonConfig\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;31m# XXX remove\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'base' is not defined"
     ]
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "frames = list()\n",
    "for _ in range(200):\n",
    "    action = 0 if np.matmul(bestparams,observation) < 0 else 1\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    \n",
    "    frame = env.render(mode = 'rgb_array')\n",
    "    frames.append(frame)\n",
    "    \n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a list with frames we can render them as a GIF. I use the following function to construct the gif and show it inline in our Jupyter notebook: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "\n",
    "def display_frames_as_gif(frames, filename_gif = None):\n",
    "    \"\"\"\n",
    "    Displays a list of frames as a gif, with controls\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "    if filename_gif: \n",
    "        anim.save(filename_gif, writer = 'imagemagick', fps=20)\n",
    "    display(display_animation(anim, default_mode='loop'))\n",
    "\n",
    "    \n",
    "display_frames_as_gif(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Video 3: Reinforcement learning explained\n",
    "Now that we are able to find the best parameters for this simple problem we can also realize that this approach is not going to work on a more interesting problem. That's why we are going to learn what [reinforcement learning](https://en.wikipedia.org/wiki/Reinforcement_learning), and specifically [q-learning](https://en.wikipedia.org/wiki/Q-learning), can do!\n",
    "\n",
    "Imagine that you are playing a game of super mario brothers. This game gives you certain rewards when you: \n",
    "- Grab a coin\n",
    "- Kill a goomba\n",
    "- Finish a level\n",
    "\n",
    "It also gives you penalties whener you fall off a cliff or walk against an enemy... \n",
    "\n",
    "The first time you played the game you simply pressed random buttons, and learned after you got penalties and rewards after you accidentally performed something good. \n",
    "\n",
    "The idea of reinforcement learning is letting an agent perform random actions and learn from them. To do this we construct a function that can determine the quality of a state-action pair. \n",
    "\n",
    "Let's say you are in a certain state. For each action you can estimate the reward you expect to get performing that action. In addition to this estimated reward we want to be able to obtain a high reward in the long term. Jumping off a cliff to grab that one extra coin is not worth it if you can walk a bit further and gain a lot of coins without dying!\n",
    "\n",
    "The function that estimates the (future) reward for an action and a state is indicated with Q(s,a). We can represent this with a neural network that takes a state as input and estimates the (future) reward of that state for each action. In case of the cartpole environment we take 4 values as input, and estimate two values (two possible actions) as output...\n",
    "\n",
    "We update this function by looking at what state we reached after performing a certain action. As we now have a new state we assume our function is correct, and the future reward can be estimated with Q(next_s, a), taking the value of a which leads to the highest reward. We multiply the next state with a discount-constant gamma. A high gamma (close to 1) values long-term rewards, while a low gamma (closer to 0) only values short-term rewards...\n",
    "\n",
    "The mathematical expression we use to update our q-function is: \n",
    "\n",
    "![reward fucntion](http://www.pinchofintelligence.com/wp-content/uploads/2017/07/rewardfunction.png)\n",
    "\n",
    "### Implement it in TensorFlow\n",
    "There are multiple ways to construct the reward-estimation function. In our case we are going to build a neural network with TensorFlow that estimates the expected (future)reward. \n",
    "\n",
    "To do this we need three input placeholders: the state, action our network took, and the reward it eventually got. This reward it eventually got is what we get from the same network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "networkstate = tf.placeholder(tf.float32, [None, 4], name=\"input\")\n",
    "networkaction = tf.placeholder(tf.int32, [None], name=\"actioninput\")\n",
    "networkreward = tf.placeholder(tf.float32,[None], name=\"groundtruth_reward\")\n",
    "\n",
    "layer1 = tf.layers.dense(networkstate, 16, activation=tf.nn.relu, name=\"dense1\")\n",
    "layer2 = tf.layers.dense(layer1, 32, activation=tf.nn.relu, name=\"dense2\")\n",
    "layer3 = tf.layers.dense(layer2, 8, activation=tf.nn.relu, name=\"dense3\")\n",
    "predictedreward = tf.layers.dense(layer3, 2, name=\"dense4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"lossdeterminer\"):\n",
    "    action_onehot = tf.one_hot(networkaction, 2, name=\"actiononehot\")\n",
    "    qreward = tf.reduce_sum(tf.multiply(predictedreward, action_onehot), reduction_indices = 1)\n",
    "    loss = tf.reduce_mean(tf.square(networkreward - qreward))\n",
    "\n",
    "tf.summary.scalar('loss', loss)\n",
    "tf.summary.scalar('mean_max_reward', tf.reduce_max(predictedreward))\n",
    "\n",
    "#optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "optimizer = tf.train.RMSPropOptimizer(0.01).minimize(loss)\n",
    "merged_summary = tf.summary.merge_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "summary_writer = tf.summary.FileWriter('graphs/dqn/normal/'+str(logging_dir_n),sess.graph)\n",
    "logging_dir_n +=1\n",
    "sess.run(tf.global_variables_initializer())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.9\n",
    "MAX_LEN_REPLAY_MEMORY = 10000 # replay memory of one million most recent frames.\n",
    "FRAMES_TO_PLAY = 50001\n",
    "MIN_FRAMES_FOR_LEARNING = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learn(replay_memory):\n",
    "      ### Learn once we have enough frames to start learning\n",
    "    if len(replay_memory) > MIN_FRAMES_FOR_LEARNING: \n",
    "        experiences = random.sample(replay_memory, BATCH_SIZE)\n",
    "        totrain = [] # (state, action, delayed_reward)\n",
    "        \n",
    "        ### Calculate the predicted reward\n",
    "        nextstates = [var[4] for var in experiences]\n",
    "        pred_reward = sess.run(predictedreward, feed_dict={networkstate:nextstates})\n",
    "        \n",
    "        ### Set the \"ground truth\": the value our network has to predict:\n",
    "        for index in range(BATCH_SIZE):\n",
    "            state, action, reward, terminalstate, newstate = experiences[index]\n",
    "            predicted_reward = max(pred_reward[index])\n",
    "            \n",
    "            if terminalstate:\n",
    "                delayedreward = reward\n",
    "            else:\n",
    "                delayedreward = reward + GAMMA*predicted_reward\n",
    "            totrain.append((state, action, delayedreward))\n",
    "            \n",
    "        ### Feed the train batch to the algorithm \n",
    "        states = [var[0] for var in totrain]\n",
    "        actions = [var[1] for var in totrain]\n",
    "        rewards = [var[2] for var in totrain]\n",
    "        _, l, summary = sess.run([optimizer, loss, merged_summary], feed_dict={networkstate:states, networkaction: actions, networkreward: rewards})\n",
    "\n",
    "\n",
    "        ### If our memory is too big: remove the first element\n",
    "        if len(replay_memory) > MAX_LEN_REPLAY_MEMORY:\n",
    "                replay_memory = replay_memory[1:]\n",
    "\n",
    "        ### Show the progress \n",
    "        summary_writer.add_summary(summary, i_epoch)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running this in our environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "replay_memory = [] # (state, action, reward, terminalstate, state_t+1)\n",
    "\n",
    "for i_epoch in range(FRAMES_TO_PLAY):\n",
    "    \n",
    "    ### Select an action and perform this\n",
    "    action = env.action_space.sample() \n",
    "    newobservation, reward, terminal, info = env.step(action)\n",
    "\n",
    "    ### I prefer that my agent gets 0 reward if it dies\n",
    "    if terminal: \n",
    "        reward = 0\n",
    "        \n",
    "    ### Add the observation to our replay memory\n",
    "    replay_memory.append((observation, action, reward, terminal, newobservation))\n",
    "    \n",
    "    ### Reset the environment if the agent died\n",
    "    if terminal: \n",
    "        newobservation = env.reset()\n",
    "    observation = newobservation\n",
    "    \n",
    "    learn(replay_memory)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now test this\n",
    "\n",
    "RUN pip install git+https://github.com/jakevdp/JSAnimation.git\n",
    "\n",
    "Use imagemagick, \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "\n",
    "def display_frames_as_gif(frames, filename_gif = None):\n",
    "    \"\"\"\n",
    "    Displays a list of frames as a gif, with controls\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "    if filename_gif: \n",
    "        anim.save(filename_gif, writer = 'imagemagick', fps=20)\n",
    "    display(display_animation(anim, default_mode='loop'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "term = False\n",
    "predicted_q = []\n",
    "frames = []\n",
    "\n",
    "### Play till we are dead\n",
    "while not term:\n",
    "    rgb_observation = env.render(mode = 'rgb_array')\n",
    "    frames.append(rgb_observation)\n",
    "    pred_q = sess.run(predictedreward, feed_dict={networkstate:[observation]})\n",
    "    predicted_q.append(pred_q)\n",
    "    action = np.argmax(pred_q)\n",
    "    observation, _, term, _ = env.step(action)\n",
    "\n",
    "### Plot the replay!\n",
    "print(\"Frames: \" + str(len(frames)))\n",
    "display_frames_as_gif(frames,filename_gif='dqn_run.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement learning tricks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "summary_writer = tf.summary.FileWriter('graphs/dqn/normal/'+str(logging_dir_n),sess.graph)\n",
    "logging_dir_n +=1\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "replay_memory = [] # (state, action, reward, terminalstate, state_t+1)\n",
    "epsilon = 1.0\n",
    "decay_factor = 0.001\n",
    "MIN_EPSILON = 0.1\n",
    "\n",
    "observation = env.reset()\n",
    "\n",
    "for i_epoch in range(FRAMES_TO_PLAY):\n",
    "    \n",
    "    ### Select action\n",
    "    if random.random() < epsilon:\n",
    "        action = env.action_space.sample() \n",
    "    else:\n",
    "        pred_q = sess.run(predictedreward, feed_dict={networkstate:[observation]})\n",
    "        action = np.argmax(pred_q)\n",
    "    \n",
    "    ### Anneal epsilon\n",
    "    epsilon = max(epsilon-decay_factor, MIN_EPSILON)\n",
    "    \n",
    "    newobservation, reward, terminal, info = env.step(action)\n",
    "    ### I prefer that my agent gets 0 reward if it dies\n",
    "    if terminal: \n",
    "        reward = 0\n",
    "        \n",
    "    ### Add the observation to our replay memory\n",
    "    replay_memory.append((observation, action, reward, terminal, newobservation))\n",
    "    \n",
    "    ### Reset the environment if the agent died\n",
    "    if terminal: \n",
    "        newobservation = env.reset()\n",
    "    observation = newobservation\n",
    "    \n",
    "    learn(replay_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "term = False\n",
    "predicted_q = []\n",
    "frames = []\n",
    "\n",
    "### Play till we are dead\n",
    "while not term:\n",
    "    rgb_observation = env.render(mode = 'rgb_array')\n",
    "    frames.append(rgb_observation)\n",
    "    pred_q = sess.run(predictedreward, feed_dict={networkstate:[observation]})\n",
    "    predicted_q.append(pred_q)\n",
    "    action = np.argmax(pred_q)\n",
    "    observation, _, term, _ = env.step(action)\n",
    "\n",
    "### Plot the replay!\n",
    "print(\"Frames: \" + str(len(frames)))\n",
    "display_frames_as_gif(frames,filename_gif='dqn_run.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video 4: Remembering your actions: adding a replay memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video 5: Introduction to the Atari environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"BreakoutDeterministic-v4\")\n",
    "observation = env.reset()\n",
    "print(observation.shape)\n",
    "plt.imshow(observation)\n",
    "\n",
    "observation = env.reset()\n",
    "for _ in range(10):\n",
    "    action = env.action_space.sample()\n",
    "    print(action)\n",
    "    observation, reward, done, info = env.step(action) \n",
    "    plt.imshow(observation)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "observation.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def huber_loss(x, delta=1.0):\n",
    "    \"\"\"Reference: https://en.wikipedia.org/wiki/Huber_loss\"\"\"\n",
    "    return tf.where(\n",
    "        tf.abs(x) < delta,\n",
    "        tf.square(x) * 0.5,\n",
    "        delta * (tf.abs(x) - 0.5 * delta)\n",
    "    )\n",
    "\n",
    "def scope_vars(scope, trainable_only=False):\n",
    "    \"\"\"\n",
    "    Get variables inside a scope\n",
    "    The scope can be specified as a string\n",
    "    Parameters\n",
    "    ----------\n",
    "    scope: str or VariableScope\n",
    "        scope in which the variables reside.\n",
    "    trainable_only: bool\n",
    "        whether or not to return only the variables that were marked as trainable.\n",
    "    Returns\n",
    "    -------\n",
    "    vars: [tf.Variable]\n",
    "        list of variables in `scope`.\n",
    "    \"\"\"\n",
    "    return tf.get_collection(\n",
    "        tf.GraphKeys.TRAINABLE_VARIABLES if trainable_only else tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "        scope=scope if isinstance(scope, str) else scope.name\n",
    "    )\n",
    "\n",
    "def scope_name():\n",
    "    \"\"\"Returns the name of current scope as a string, e.g. deepq/q_func\"\"\"\n",
    "    return tf.get_variable_scope().name\n",
    "\n",
    "def absolute_scope_name(relative_scope_name):\n",
    "    \"\"\"Appends parent scope name to `relative_scope_name`\"\"\"\n",
    "    return scope_name() + \"/\" + relative_scope_name\n",
    "\n",
    "\n",
    "def function(inputs, outputs, updates=None, givens=None):\n",
    "    \"\"\"Just like Theano function. Take a bunch of tensorflow placeholders and expressions\n",
    "    computed based on those placeholders and produces f(inputs) -> outputs. Function f takes\n",
    "    values to be fed to the input's placeholders and produces the values of the expressions\n",
    "    in outputs.\n",
    "    Input values can be passed in the same order as inputs or can be provided as kwargs based\n",
    "    on placeholder name (passed to constructor or accessible via placeholder.op.name).\n",
    "    Example:\n",
    "        x = tf.placeholder(tf.int32, (), name=\"x\")\n",
    "        y = tf.placeholder(tf.int32, (), name=\"y\")\n",
    "        z = 3 * x + 2 * y\n",
    "        lin = function([x, y], z, givens={y: 0})\n",
    "        with single_threaded_session():\n",
    "            initialize()\n",
    "            assert lin(2) == 6\n",
    "            assert lin(x=3) == 9\n",
    "            assert lin(2, 2) == 10\n",
    "            assert lin(x=2, y=3) == 12\n",
    "    Parameters\n",
    "    ----------\n",
    "    inputs: [tf.placeholder or TfInput]\n",
    "        list of input arguments\n",
    "    outputs: [tf.Variable] or tf.Variable\n",
    "        list of outputs or a single output to be returned from function. Returned\n",
    "        value will also have the same shape.\n",
    "    \"\"\"\n",
    "    if isinstance(outputs, list):\n",
    "        return _Function(inputs, outputs, updates, givens=givens)\n",
    "    elif isinstance(outputs, (dict, collections.OrderedDict)):\n",
    "        f = _Function(inputs, outputs.values(), updates, givens=givens)\n",
    "        return lambda *args, **kwargs: type(outputs)(zip(outputs.keys(), f(*args, **kwargs)))\n",
    "    else:\n",
    "        f = _Function(inputs, [outputs], updates, givens=givens)\n",
    "        return lambda *args, **kwargs: f(*args, **kwargs)[0]\n",
    "\n",
    "class _Function(object):\n",
    "    def __init__(self, inputs, outputs, updates, givens, check_nan=False):\n",
    "        for inpt in inputs:\n",
    "            if not issubclass(type(inpt), TfInput):\n",
    "                assert len(inpt.op.inputs) == 0, \"inputs should all be placeholders of baselines.common.TfInput\"\n",
    "        self.inputs = inputs\n",
    "        updates = updates or []\n",
    "        self.update_group = tf.group(*updates)\n",
    "        self.outputs_update = list(outputs) + [self.update_group]\n",
    "        self.givens = {} if givens is None else givens\n",
    "        self.check_nan = check_nan\n",
    "\n",
    "    def _feed_input(self, feed_dict, inpt, value):\n",
    "        if issubclass(type(inpt), TfInput):\n",
    "            feed_dict.update(inpt.make_feed_dict(value))\n",
    "        elif is_placeholder(inpt):\n",
    "            feed_dict[inpt] = value\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        assert len(args) <= len(self.inputs), \"Too many arguments provided\"\n",
    "        feed_dict = {}\n",
    "        # Update the args\n",
    "        for inpt, value in zip(self.inputs, args):\n",
    "            self._feed_input(feed_dict, inpt, value)\n",
    "        # Update the kwargs\n",
    "        kwargs_passed_inpt_names = set()\n",
    "        for inpt in self.inputs[len(args):]:\n",
    "            inpt_name = inpt.name.split(':')[0]\n",
    "            inpt_name = inpt_name.split('/')[-1]\n",
    "            assert inpt_name not in kwargs_passed_inpt_names, \\\n",
    "                \"this function has two arguments with the same name \\\"{}\\\", so kwargs cannot be used.\".format(inpt_name)\n",
    "            if inpt_name in kwargs:\n",
    "                kwargs_passed_inpt_names.add(inpt_name)\n",
    "                self._feed_input(feed_dict, inpt, kwargs.pop(inpt_name))\n",
    "            else:\n",
    "                assert inpt in self.givens, \"Missing argument \" + inpt_name\n",
    "        assert len(kwargs) == 0, \"Function got extra arguments \" + str(list(kwargs.keys()))\n",
    "        # Update feed dict with givens.\n",
    "        for inpt in self.givens:\n",
    "            feed_dict[inpt] = feed_dict.get(inpt, self.givens[inpt])\n",
    "        results = get_session().run(self.outputs_update, feed_dict=feed_dict)[:-1]\n",
    "        if self.check_nan:\n",
    "            if any(np.isnan(r).any() for r in results):\n",
    "                raise RuntimeError(\"Nan detected\")\n",
    "        return results\n",
    "\n",
    "def mem_friendly_function(nondata_inputs, data_inputs, outputs, batch_size):\n",
    "    if isinstance(outputs, list):\n",
    "        return _MemFriendlyFunction(nondata_inputs, data_inputs, outputs, batch_size)\n",
    "    else:\n",
    "        f = _MemFriendlyFunction(nondata_inputs, data_inputs, [outputs], batch_size)\n",
    "        return lambda *inputs: f(*inputs)[0]\n",
    "\n",
    "class _MemFriendlyFunction(object):\n",
    "    def __init__(self, nondata_inputs, data_inputs, outputs, batch_size):\n",
    "        self.nondata_inputs = nondata_inputs\n",
    "        self.data_inputs = data_inputs\n",
    "        self.outputs = list(outputs)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __call__(self, *inputvals):\n",
    "        assert len(inputvals) == len(self.nondata_inputs) + len(self.data_inputs)\n",
    "        nondata_vals = inputvals[0:len(self.nondata_inputs)]\n",
    "        data_vals = inputvals[len(self.nondata_inputs):]\n",
    "        feed_dict = dict(zip(self.nondata_inputs, nondata_vals))\n",
    "        n = data_vals[0].shape[0]\n",
    "        for v in data_vals[1:]:\n",
    "            assert v.shape[0] == n\n",
    "        for i_start in range(0, n, self.batch_size):\n",
    "            slice_vals = [v[i_start:builtins.min(i_start + self.batch_size, n)] for v in data_vals]\n",
    "            for (var, val) in zip(self.data_inputs, slice_vals):\n",
    "                feed_dict[var] = val\n",
    "            results = tf.get_default_session().run(self.outputs, feed_dict=feed_dict)\n",
    "            if i_start == 0:\n",
    "                sum_results = results\n",
    "            else:\n",
    "                for i in range(len(results)):\n",
    "                    sum_results[i] = sum_results[i] + results[i]\n",
    "        for i in range(len(results)):\n",
    "            sum_results[i] = sum_results[i] / n\n",
    "        return sum_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "\n",
    "tf.reset_default_graph()\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "networkstate = tf.placeholder(tf.float32, [None, 84, 84, 4], name=\"input\")\n",
    "nextnetworkstate = tf.placeholder(tf.float32, [None, 84, 84, 4], name=\"inputnext\")\n",
    "networkaction = tf.placeholder(tf.int32, [None], name=\"actioninput\")\n",
    "networkreward = tf.placeholder(tf.float32, [None], name=\"actioninput\")\n",
    "done_mask_ph = tf.placeholder(tf.float32, [None], name=\"done\")\n",
    "\n",
    "\n",
    "inputimage = networkstate/255\n",
    "nextnetworkstate = nextnetworkstate/255\n",
    "# inputimage = tf.image.resize_images(inputimage, (84, 84))\n",
    "\n",
    "#tf.summary.image(\"inputimage\", inputimage)\n",
    "\n",
    "def build_network(namescope, input_for_network):\n",
    "    with tf.variable_scope(namescope):\n",
    "        layer1 = tf.layers.conv2d(inputimage, 32, [8,8], activation=tf.nn.relu, strides=(4, 4), name=\"conv1\", padding='same')\n",
    "        layer2 = tf.layers.conv2d(layer1, 64, [4,4], strides=(2,2), activation=tf.nn.relu, name=\"conv2\", padding='same')\n",
    "        layer3 = tf.layers.conv2d(layer2, 64, [3,3], activation=tf.nn.relu, name=\"conv3\", padding=\"same\")\n",
    "        flattened = tf.contrib.layers.flatten(layer3)\n",
    "\n",
    "        flat2 = tf.layers.dense(flattened, 256, activation=tf.nn.relu, name=\"dense2\")\n",
    "        predictedreward = tf.layers.dense(flat2, env.action_space.n, name=\"dense3\")\n",
    "    return predictedreward\n",
    "\n",
    "predicted_action = build_network(\"actionnetwork\", inputimage)\n",
    "#q_func_vars = scope_vars(absolute_scope_name(\"actionnetwork\"))\n",
    "q_func_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"actionnetwork\")\n",
    "assert len(q_func_vars) > 0\n",
    "\n",
    "predicted_value = build_network(\"valuenetwork\", nextnetworkstate)\n",
    "#target_q_func_vars = scope_vars(absolute_scope_name(\"valuenetwork\"))\n",
    "target_q_func_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"valuenetwork\")\n",
    "assert len(target_q_func_vars) > 0\n",
    "\n",
    "with tf.name_scope(\"lossdeterminer\"):\n",
    "    action_onehot = tf.one_hot(networkaction, env.action_space.n, name=\"actiononehot\")\n",
    "    predicted_qreward = tf.reduce_sum(tf.multiply(predicted_action, action_onehot), reduction_indices = 1)\n",
    "    ## TODO: Do I want to set estimated reward to zero if the agent was done in this frame?\n",
    "    estimated_reward = tf.reduce_max(predicted_value)\n",
    "\n",
    "    q_tp1_best_masked = (1.0 - done_mask_ph) * estimated_reward\n",
    "\n",
    "    q_t_selected_target = networkreward + GAMMA * q_tp1_best_masked\n",
    "    td_error = predicted_qreward - tf.stop_gradient(q_t_selected_target)\n",
    "    loss = huber_loss(td_error)\n",
    "\n",
    "#     loss = tf.reduce_mean(tf.square(networkreward - qreward))\n",
    "\n",
    "\n",
    "sum1 = tf.summary.scalar('loss', tf.reduce_sum(loss))\n",
    "sum2 = tf.summary.scalar('mean_max_reward', tf.reduce_max(predicted_action))\n",
    "merged_summary = tf.summary.merge([sum1, sum2]) #tf.summary.merge_all()\n",
    "\n",
    "\n",
    "#optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "#optimizer = tf.train.RMSPropOptimizer(0.001).minimize(loss)\n",
    "learning_rate_minimum = 0.00015\n",
    "learning_rate = 0.0025\n",
    "learning_rate_decay = 0.97\n",
    "scale = 10000\n",
    "learning_rate_decay_step = 5 * scale\n",
    "\n",
    "learning_rate_step = tf.placeholder('int64', None, name='learning_rate_step')\n",
    "learning_rate_op = tf.maximum(learning_rate_minimum,tf.train.exponential_decay(\n",
    "      learning_rate,\n",
    "      global_step,\n",
    "      learning_rate_decay_step,\n",
    "      learning_rate_decay,\n",
    "      staircase=True))\n",
    "\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate_op, momentum=0.97, epsilon=0.001).minimize(loss, global_step=global_step)\n",
    "\n",
    "\n",
    "mean_last_n_ph = tf.placeholder(tf.float32)\n",
    "max_last_n_ph = tf.placeholder(tf.float32)\n",
    "min_last_n_ph = tf.placeholder(tf.float32)\n",
    "epsilon_ph = tf.placeholder(tf.float32)\n",
    "sum3 = tf.summary.scalar(\"mean_last_n\",mean_last_n_ph )\n",
    "sum4 = tf.summary.scalar(\"max_last_n\",max_last_n_ph )\n",
    "sum5 = tf.summary.scalar(\"min_last_n\",min_last_n_ph )\n",
    "sum6 = tf.summary.scalar(\"epsilon\",epsilon_ph )\n",
    "sum7 = tf.summary.scalar(\"learning_rate\",learning_rate_op )\n",
    "merged_summary_meta = tf.summary.merge([sum3, sum4, sum5, sum6, sum7])\n",
    "\n",
    "update_target_expr = []\n",
    "for var, var_target in zip(sorted(q_func_vars, key=lambda v: v.name), sorted(target_q_func_vars, key=lambda v: v.name)):\n",
    "    update_target_expr.append(var_target.assign(var))\n",
    "update_target_expr = tf.group(*update_target_expr)\n",
    "update_target = function([], [], updates=[update_target_expr])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(q_func_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "summary_writer = tf.summary.FileWriter('graphs/dqn/atari/'+str(logging_dir_n),sess.graph)\n",
    "logging_dir_n +=1\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "def preprocess(observation):\n",
    "    observation = cv2.cvtColor(cv2.resize(observation, (84, 110), interpolation = cv2.INTER_AREA), cv2.COLOR_BGR2GRAY)\n",
    "    observation = observation[26:110,:]\n",
    "    ret, observation = cv2.threshold(observation,1,255,cv2.THRESH_BINARY)\n",
    "    return np.reshape(observation,(84,84,1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import collections\n",
    "replay_memory = [] # (state, action, reward, terminalstate, state_t+1)\n",
    "epsilon = 1.0\n",
    "decay_factor = 0.000001\n",
    "MIN_EPSILON = 0.1\n",
    "BATCH_SIZE = 32\n",
    "MAX_LEN_REPLAY_MEMORY = 500000 # replay memory of one million most recent frames.\n",
    "FRAMES_TO_PLAY = 50000000\n",
    "MIN_FRAMES_FOR_LEARNING = 1000\n",
    "\n",
    "observation = env.reset()\n",
    "observation = preprocess(observation)\n",
    "print(observation.shape)\n",
    "plt.imshow(np.reshape(observation, (84,84)), cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "total_reward_this_episode = 0\n",
    "best_reward_so_far = 0\n",
    "lastframes = collections.deque([observation]*4, 4)\n",
    "\n",
    "def mean(numbers):\n",
    "    return float(sum(numbers)) / max(len(numbers), 1)\n",
    "\n",
    "last_rewards = list()\n",
    "last_n = 100\n",
    "mean_last_n = 0\n",
    "min_last_n = 0\n",
    "max_last_n = 0\n",
    "\n",
    "\n",
    "for _ in range(4):\n",
    "    action = env.action_space.sample() \n",
    "    newobservation, reward, terminal, info = env.step(action)\n",
    "    lastframes.append(preprocess(newobservation))\n",
    "observation = np.dstack(lastframes)    \n",
    "for i_epoch in range(FRAMES_TO_PLAY):\n",
    "    \n",
    "    if i_epoch %10000 == 10:\n",
    "        print(\"SETTING NETWORK PARAMETERS TO SAME\" + str(i_epoch) + \" \" + str(epsilon))\n",
    "        sess.run(update_target_expr)\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### Select action\n",
    "    if i_epoch%50==0:\n",
    "        action = 1 # Press start once in a while\n",
    "    elif random.random() < epsilon:\n",
    "        action = env.action_space.sample() \n",
    "    else:\n",
    "        pred_q = sess.run(predicted_action, feed_dict={networkstate:[observation]})\n",
    "       # print(pred_q)\n",
    "\n",
    "        action = np.argmax(pred_q)\n",
    "    \n",
    "    ### Anneal epsilon\n",
    "    epsilon = max(epsilon-decay_factor, MIN_EPSILON)\n",
    "    \n",
    "    newobservation, reward, terminal, info = env.step(action)\n",
    "    lastframes.appendleft(preprocess(newobservation))\n",
    "    newobservation = np.dstack(lastframes)\n",
    "   \n",
    "#     plt.imshow(newobservation[:,:,:3])\n",
    "#     plt.show()\n",
    "    \n",
    "    ### I prefer that my agent gets 0 reward if it dies\n",
    "    if terminal: \n",
    "        reward = 0\n",
    "    \n",
    "    total_reward_this_episode += reward\n",
    "        \n",
    "    ### Add the observation to our replay memory\n",
    "    replay_memory.append((observation, action, reward, terminal, newobservation))\n",
    "    \n",
    "    ### Reset the environment if the agent died\n",
    "    if terminal:\n",
    "        if total_reward_this_episode > best_reward_so_far:\n",
    "            best_reward_so_far = total_reward_this_episode\n",
    "            print(\"Reward this episode: \" + str(total_reward_this_episode) + \" epsilon: \" + str(epsilon))\n",
    "        last_rewards.append(total_reward_this_episode)\n",
    "        \n",
    " \n",
    "        mean_last_n = mean(last_rewards[-last_n:])\n",
    "    \n",
    "        min_last_n = min(last_rewards[-last_n:])\n",
    "        max_last_n = max(last_rewards[-last_n:])\n",
    "        total_reward_this_episode = 0\n",
    "        newobservation = env.reset()\n",
    "        lastframes.appendleft(preprocess(newobservation))\n",
    "        newobservation = np.dstack(lastframes)\n",
    "    observation = newobservation\n",
    "    \n",
    "    ### Learn once we have enough frames to start learning\n",
    "    if len(replay_memory) > MIN_FRAMES_FOR_LEARNING: \n",
    "        experiences = random.sample(replay_memory, BATCH_SIZE)\n",
    "     \n",
    "        totrain = [] # (state, action, delayed_reward)\n",
    "        \n",
    "        ### Calculate the predicted reward\n",
    "\n",
    "#         nextstates = [var[4] for var in experiences]\n",
    "#         currentstates = [var[0] for var in experiences]\n",
    "#         performedactions = [var[1] for var in experiences]\n",
    "        \n",
    "        \n",
    "        ### Set the \"ground truth\": the value our network has to predict:\n",
    "#         for index in range(BATCH_SIZE):\n",
    "#             state, action, reward, terminalstate, newstate = experiences[index]\n",
    "#             #print(terminalstate)\n",
    "#             #totrain.append((state, action, delayedreward))\n",
    "            \n",
    "        ### Feed the train batch to the algorithm \n",
    "        states = [var[0] for var in experiences]\n",
    "        actions = [var[1] for var in experiences]\n",
    "        rewards = [var[2] for var in experiences]\n",
    "        terminalstates = [float(var[3]) for var in experiences]\n",
    "        nextstates = [var[4] for var in experiences]\n",
    "        \n",
    "      #  print(terminalstates)\n",
    "      #  print(nextstates)\n",
    "        \n",
    "        _, l, summary = sess.run([optimizer, loss, merged_summary], \n",
    "                                 feed_dict={networkstate:states, \n",
    "                                            networkaction: actions, \n",
    "                                            networkreward: rewards, \n",
    "                                           nextnetworkstate: nextstates, \n",
    "                                           done_mask_ph: terminalstates})\n",
    "\n",
    "   \n",
    "        ### If our memory is too big: remove the first element\n",
    "        if len(replay_memory) > MAX_LEN_REPLAY_MEMORY:\n",
    "                replay_memory = replay_memory[1:]\n",
    "\n",
    "        ### Show the progress \n",
    "        summary_writer.add_summary(summary, i_epoch)\n",
    "        \n",
    "        \n",
    "        summary = sess.run(merged_summary_meta, feed_dict={mean_last_n_ph: mean_last_n, \n",
    "                                                           max_last_n_ph: max_last_n, \n",
    "                                                           min_last_n_ph: min_last_n,\n",
    "                                                          epsilon_ph: epsilon})\n",
    "#         summary = sess.run(merged_summary_meta, feed_dict={mean_last_n_ph: mean_last_n, \n",
    "#                                                           max_last_n_ph: float(max_last_n), \n",
    "#                                                           min_last_n: float(min_last_n),\n",
    "#                                                           epsilon_ph: epsilon})\n",
    "        summary_writer.add_summary(summary, i_epoch)\n",
    "       \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i_epoch in range(5000000, 50000000):\n",
    "    \n",
    "    if i_epoch %10000 == 10:\n",
    "        print(\"SETTING NETWORK PARAMETERS TO SAME\" + str(i_epoch) + \" \" + str(epsilon))\n",
    "        sess.run(update_target_expr)\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### Select action\n",
    "    if i_epoch%50==0:\n",
    "        action = 1 # Press start once in a while\n",
    "    elif random.random() < epsilon:\n",
    "        action = env.action_space.sample() \n",
    "    else:\n",
    "        pred_q = sess.run(predicted_action, feed_dict={networkstate:[observation]})\n",
    "       # print(pred_q)\n",
    "\n",
    "        action = np.argmax(pred_q)\n",
    "    \n",
    "    ### Anneal epsilon\n",
    "    epsilon = max(epsilon-decay_factor, MIN_EPSILON)\n",
    "    \n",
    "    newobservation, reward, terminal, info = env.step(action)\n",
    "    lastframes.appendleft(preprocess(newobservation))\n",
    "    newobservation = np.dstack(lastframes)\n",
    "   \n",
    "#     plt.imshow(newobservation[:,:,:3])\n",
    "#     plt.show()\n",
    "    \n",
    "    ### I prefer that my agent gets 0 reward if it dies\n",
    "    if terminal: \n",
    "        reward = 0\n",
    "    \n",
    "    total_reward_this_episode += reward\n",
    "        \n",
    "    ### Add the observation to our replay memory\n",
    "    replay_memory.append((observation, action, reward, terminal, newobservation))\n",
    "    \n",
    "    ### Reset the environment if the agent died\n",
    "    if terminal:\n",
    "        if total_reward_this_episode > best_reward_so_far:\n",
    "            best_reward_so_far = total_reward_this_episode\n",
    "            print(\"Reward this episode: \" + str(total_reward_this_episode) + \" epsilon: \" + str(epsilon))\n",
    "        last_rewards.append(total_reward_this_episode)\n",
    "        \n",
    " \n",
    "        mean_last_n = mean(last_rewards[-last_n:])\n",
    "    \n",
    "        min_last_n = min(last_rewards[-last_n:])\n",
    "        max_last_n = max(last_rewards[-last_n:])\n",
    "        total_reward_this_episode = 0\n",
    "        newobservation = env.reset()\n",
    "        lastframes.appendleft(preprocess(newobservation))\n",
    "        newobservation = np.dstack(lastframes)\n",
    "    observation = newobservation\n",
    "    \n",
    "    ### Learn once we have enough frames to start learning\n",
    "    if len(replay_memory) > MIN_FRAMES_FOR_LEARNING: \n",
    "        experiences = random.sample(replay_memory, BATCH_SIZE)\n",
    "     \n",
    "        totrain = [] # (state, action, delayed_reward)\n",
    "        \n",
    "        ### Calculate the predicted reward\n",
    "\n",
    "#         nextstates = [var[4] for var in experiences]\n",
    "#         currentstates = [var[0] for var in experiences]\n",
    "#         performedactions = [var[1] for var in experiences]\n",
    "        \n",
    "        \n",
    "        ### Set the \"ground truth\": the value our network has to predict:\n",
    "#         for index in range(BATCH_SIZE):\n",
    "#             state, action, reward, terminalstate, newstate = experiences[index]\n",
    "#             #print(terminalstate)\n",
    "#             #totrain.append((state, action, delayedreward))\n",
    "            \n",
    "        ### Feed the train batch to the algorithm \n",
    "        states = [var[0] for var in experiences]\n",
    "        actions = [var[1] for var in experiences]\n",
    "        rewards = [var[2] for var in experiences]\n",
    "        terminalstates = [float(var[3]) for var in experiences]\n",
    "        nextstates = [var[4] for var in experiences]\n",
    "        \n",
    "      #  print(terminalstates)\n",
    "      #  print(nextstates)\n",
    "        \n",
    "        _, l, summary = sess.run([optimizer, loss, merged_summary], \n",
    "                                 feed_dict={networkstate:states, \n",
    "                                            networkaction: actions, \n",
    "                                            networkreward: rewards, \n",
    "                                           nextnetworkstate: nextstates, \n",
    "                                           done_mask_ph: terminalstates})\n",
    "\n",
    "   \n",
    "        ### If our memory is too big: remove the first element\n",
    "        if len(replay_memory) > MAX_LEN_REPLAY_MEMORY:\n",
    "                replay_memory = replay_memory[1:]\n",
    "\n",
    "        ### Show the progress \n",
    "        summary_writer.add_summary(summary, i_epoch)\n",
    "        \n",
    "        \n",
    "        summary = sess.run(merged_summary_meta, feed_dict={mean_last_n_ph: mean_last_n, \n",
    "                                                           max_last_n_ph: max_last_n, \n",
    "                                                           min_last_n_ph: min_last_n,\n",
    "                                                          epsilon_ph: epsilon})\n",
    "#         summary = sess.run(merged_summary_meta, feed_dict={mean_last_n_ph: mean_last_n, \n",
    "#                                                           max_last_n_ph: float(max_last_n), \n",
    "#                                                           min_last_n: float(min_last_n),\n",
    "#                                                           epsilon_ph: epsilon})\n",
    "        summary_writer.add_summary(summary, i_epoch)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(replay_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "term = False\n",
    "predicted_q = []\n",
    "frames = []\n",
    "\n",
    "lastframes.appendleft(preprocess(observation))\n",
    "newobservation = np.dstack(lastframes)\n",
    "   \n",
    "### Play till we are dead\n",
    "for _ in range(300):\n",
    "#while not term:\n",
    "    rgb_observation = env.render(mode = 'rgb_array')\n",
    "    frames.append(rgb_observation)\n",
    "    \n",
    "    \n",
    "    lastframes.appendleft(preprocess(newobservation))\n",
    "    newobservation = np.dstack(lastframes)\n",
    "    print(newobservation.max())\n",
    "        \n",
    "    pred_q = sess.run(predicted_action, feed_dict={networkstate:[newobservation]})\n",
    "    predicted_q.append(pred_q)\n",
    "    \n",
    "    if random.random() < 0.2:\n",
    "        action = env.action_space.sample() \n",
    "    else:\n",
    "        action = np.argmax(pred_q)\n",
    "    \n",
    "    observation, _, term, _ = env.step(action)\n",
    "    if term: \n",
    "        env.reset()\n",
    "    newobservation = observation\n",
    "### Plot the replay!\n",
    "print(\"Frames: \" + str(len(frames)))\n",
    "display_frames_as_gif(frames,filename_gif='dqn_run.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for _ in range(300):\n",
    "    rgb_observation = env.render(mode = 'rgb_array')\n",
    "    plt.imshow(rgb_observation)\n",
    "    plt.show()\n",
    "    \n",
    "    action = int(input(\"input\"))\n",
    "    observation, _, term, _ = env.step(action)\n",
    "    print(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for var in target_q_func_vars:\n",
    "    print(var.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(target_q_func_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainable_only = False\n",
    "tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES if trainable_only else tf.GraphKeys.GLOBAL_VARIABLES, scope=scope if isinstance(scope, str) else scope.name\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.get_variable_scope().name\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"/\" + relative_scope_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "var_23 = [v for v in tf.global_variables() if v.name.startswith(\"actionnetwork\")]\n",
    "print(var_23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "var_23 = [v for v in tf.global_variables() if v.name.startswith(\"valuenetwork\")]\n",
    "print(var_23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"actionnetwork\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
